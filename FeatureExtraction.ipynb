{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def windows(data, window_size):\n",
    "    start = 0\n",
    "    while True:\n",
    "        yield start, start + window_size\n",
    "        if(start+ window_size >= len(data)):\n",
    "            break\n",
    "        start += (window_size // 2)\n",
    "\n",
    "#for very short wav file, I still want to keep it.\n",
    "def extract_features(path_dir, ds_filename, fold_num, n_mfcc = 20, frames = 41):\n",
    "    window_size = 512 * (frames - 1)\n",
    "    mfccs = []\n",
    "    labels = []\n",
    "    labels_name = []\n",
    "    file_name = []\n",
    "    df = pd.read_csv(ds_filename, index_col = False)\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        if(row[6]==fold_num):\n",
    "            sound_clip,s = librosa.load(path_dir + str(row[6])+\"/\" + str(row[1]))\n",
    "            \n",
    "            for (start,end) in windows(sound_clip,window_size):\n",
    "                if(len(sound_clip[start:end]) == window_size):\n",
    "                    signal = sound_clip[start:end]\n",
    "                elif (start==0):\n",
    "                    signal = np.lib.pad(sound_clip[start:], (0, window_size - len(sound_clip[start: end])), 'constant', constant_values = (0))\n",
    "                else: \n",
    "                    break\n",
    "                mfcc = librosa.feature.mfcc(y=signal, sr=s, n_mfcc = n_mfcc).T.flatten()[:, np.newaxis].T\n",
    "                mfccs.append(mfcc)\n",
    "                labels.append(row[7])\n",
    "                labels_name.append(row[8])\n",
    "                file_name.append(row[1])\n",
    "    \n",
    "    features = np.asarray(mfccs).reshape(len(mfccs),frames, n_mfcc)\n",
    "    return np.array(features), np.array(labels,dtype = np.int), np.array(labels_name), np.array(file_name)\n",
    "\n",
    "def extract_features_delta(path_dir, ds_filename, fold_num, n_mels = 60, frames = 41):\n",
    "    window_size = 512 * (frames - 1)\n",
    "    #mfccs = []\n",
    "    log_specgrams = []\n",
    "    labels = []\n",
    "    labels_name = []\n",
    "    file_name = []\n",
    "    df = pd.read_csv(ds_filename, index_col = False)\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        if(row[6]==fold_num):\n",
    "            sound_clip,s = librosa.load(path_dir + str(row[6])+\"/\" + str(row[1]))\n",
    "            \n",
    "            for (start,end) in windows(sound_clip,window_size):\n",
    "                if(len(sound_clip[start:end]) == window_size):\n",
    "                    signal = sound_clip[start:end]\n",
    "                elif (start==0):\n",
    "                    signal = np.lib.pad(sound_clip[start:], (0, window_size - len(sound_clip[start: end])), 'constant', constant_values = (0))\n",
    "                else: \n",
    "                    break\n",
    "                melspec = librosa.feature.melspectrogram(signal, n_mels = n_mels)\n",
    "                logspec = librosa.logamplitude(melspec)\n",
    "                logspec = logspec.T.flatten()[:, np.newaxis].T\n",
    "                log_specgrams.append(logspec)\n",
    "\n",
    "                labels.append(row[7])\n",
    "                labels_name.append(row[8])\n",
    "                file_name.append(row[1])\n",
    "                \n",
    "    log_specgrams = np.asarray(log_specgrams).reshape(len(log_specgrams),n_mels,frames,1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis = 3)\n",
    "    for i in range(len(features)):\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "    \n",
    "    #features = np.asarray(mfccs).reshape(len(mfccs),frames, n_mfcc)\n",
    "    return np.array(features), np.array(labels,dtype = np.int), np.array(labels_name), np.array(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mega_file = './UrbanSound8K/metadata/UrbanSound8K.csv' \n",
    "path_dir = \"./UrbanSound8K/audio/fold\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract log_specgrams and delta from wav file and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "v_n_mels = 60\n",
    "v_frames = 41\n",
    "for i in range(1, 11):\n",
    "    features, labels, labels_name, file_name = extract_features_delta(path_dir, mega_file, i, n_mels = v_n_mels, frames = v_frames)\n",
    "    feature_pd = pd.DataFrame(features.reshape((-1, v_n_mels * v_frames * 2))) \n",
    "    feature_pd[\"label\"] = labels\n",
    "    feature_pd[\"labels_name\"] = labels_name\n",
    "    feature_pd[\"file_name\"]= file_name\n",
    "    feature_pd.to_csv(\"./UrbanSound8K/audio/delta\" + str(i) + \".csv\", index = False)\n",
    "    print (\"fold\"+str(i) +\" finished transferring.\")\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"seconds:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract MFCCs from wav file and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for i in range(1, 11):\n",
    "    features, labels, labels_name, file_name = extract_features(path_dir, mega_file, i, n_mfcc = 20)\n",
    "    feature_pd = pd.DataFrame(features.reshape((-1, 820))) \n",
    "    feature_pd[\"label\"] = labels\n",
    "    feature_pd[\"labels_name\"] = labels_name\n",
    "    feature_pd[\"file_name\"]= file_name\n",
    "    feature_pd.to_csv(path_dir + str(i) + \"/\" + \"mfcc_f\" + str(i) + \".csv\", index = False)\n",
    "    print (\"fold\"+str(i) +\" finished transferring.\")\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"seconds:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract 193 features from one wav file and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feature_193(file_name):\n",
    "    X, sample_rate = librosa.load(file_name)\n",
    "    \n",
    "    \"\"\"\n",
    "    The short-time Fourier transform (STFT), or alternatively short-term Fourier transform, \n",
    "    is a Fourier-related transform used to determine the sinusoidal frequency and phase content \n",
    "    of local sections of a signal as it changes over time. In practice, the procedure for \n",
    "    computing STFTs is to divide a longer time signal into shorter segments of equal length \n",
    "    and then compute the Fourier transform separately on each shorter segment. This reveals \n",
    "    the Fourier spectrum on each shorter segment. One then usually plots the changing spectra \n",
    "    as a function of time.\n",
    "    \"\"\"\n",
    "    \n",
    "    stft = np.abs(librosa.stft(X))\n",
    "    \"\"\"\n",
    "    The most commonly used feature extraction method in automatic speech recognition (ASR) is Mel-Frequency \n",
    "    Cepstral Coefficients (MFCC) [1]. This feature extraction method was first mentioned by Bridle and Brown in 1974 \n",
    "    and further developed by Mermelstein in 1976 and is based on experiments of the human misconception of words.\n",
    "\n",
    "    To extract a feature vector containing all information about the linguistic message, MFCC mimics some parts of the \n",
    "    human speech production and speech perception. MFCC mimics the logarithmic perception of loudness and pitch of human \n",
    "    auditory system and tries to eliminate speaker dependent characteristics by excluding the fundamental frequency and \n",
    "    their harmonics. To represent the dynamic nature of speech the MFCC also includes the change of the feature vector \n",
    "    over time as part of the feature vector.\n",
    "    \"\"\"\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    \n",
    "    \"\"\"\n",
    "    Chroma features are an interesting and powerful representation for music audio in which the entire spectrum \n",
    "    is projected onto 12 bins representing the 12 distinct semitones (or chroma) of the musical octave. Since, \n",
    "    in music, notes exactly one octave apart are perceived as particularly similar, knowing the distribution \n",
    "    of chroma even without the absolute frequency (i.e. the original octave) can give useful musical information \n",
    "    about the audio -- and may even reveal perceived musical similarity that is not apparent in the original spectra.\n",
    "    \"\"\"\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    \n",
    "    \"\"\"\n",
    "    In sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum \n",
    "    of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.\n",
    "    \n",
    "    mel-scaled spectrogram\n",
    "    \"\"\"\n",
    "    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute spectral contrast\n",
    "    \"\"\"\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "    \"\"\"\n",
    "    Computes the tonal centroid features (tonnetz)\n",
    "    \"\"\"\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "    return mfccs,chroma,mel,contrast,tonnetz\n",
    "\n",
    "#concatenate the features together as the input to training algorithm\n",
    "\n",
    "def parse_audio_files(path_dir, ds_filename, fold_num):  \n",
    "    #knowledge_based feature transformation. Transform each .wav file to a list of audio features\n",
    "    features, labels, labels_name, file_name = np.empty((0,193)), np.empty(0, dtype = int), np.empty(0), np.empty(0)\n",
    "    df = pd.read_csv(ds_filename, index_col = False)\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        if(row[6]==fold_num):\n",
    "            mfccs, chroma, mel, contrast,tonnetz = extract_feature_193(path_dir + str(row[6])+\"/\" + str(row[1]))\n",
    "            \n",
    "            ext_features = np.hstack([mfccs,chroma,mel,contrast,tonnetz])\n",
    "            features = np.vstack([features,ext_features])\n",
    "            labels = np.append(labels, row[7])\n",
    "            labels_name = np.append(labels_name, row[8])\n",
    "            file_name = np.append(file_name, row[1])\n",
    " \n",
    "    return features, labels, labels_name, file_name #np.array(features), np.array(labels, dtype = np.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for i in range(1, 11):\n",
    "    f, l, ln, fn = parse_audio_files(path_dir, mega_file, i)\n",
    "    feature_pd_193 = pd.DataFrame(f) #np.array(mfcc_data).reshape(len(mfcc_data), 820))\n",
    "    feature_pd_193[\"label\"] = l\n",
    "    feature_pd_193[\"labels_name\"] = ln\n",
    "    feature_pd_193[\"file_name\"] = fn\n",
    "    feature_pd_193.to_csv(\"./UrbanSound8K/audio/features193\" + str(i) + \".csv\", index = False)\n",
    "    print (\"fold\"+str(i) +\" finished transferring.\")\n",
    "end_time = time.time()\n",
    "print(\"seconds:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
